ofekwa
roi.gernett


=============================
=      File description     =
=============================
AvlNode.java - this class represents a node in an Avltree. it has a data, a right and left son, and a father.
               also, every node has a height field, which is the hight of the tree starting with the node,
               and a balance field, which is the height of the right son minus height of the left son.
AvlTree.java - this class represents a tree. it has one field - the root node.
integerIterator.java - this class represents an iterator which goes over an AvlTree. the class implements
               Iterator<Integer>, however, it doesn't support the 'remove' operation.




=============================
=  Implementation details   =
=============================
add() - Insertion begins as a search would begin; if the key is not equal to that of the root, we search the
        left or right subtrees as before. Eventually, we will reach an external node and add the new key-value
        pair as its right or left child, depending on the node's key. In other words, we examine the root and
        recursively insert the new node to the left subtree if its key is less than that of the root,
		or the right subtree if its key is greater than or equal to the root.
		After the Insertion, the methods reheight() and rebalance() are called.
		
delete() - When removing a node from an Avl tree it is mandatory to maintain the in-order sequence of the
           nodes. It is important to note that the deletion itself is an action that is done exactly the same
           as it is done in a regular binary search tree, The difference is the actions we do after - that
           rebalance the tree afterwards and keeps it Avl property.

		   There are three possible cases to consider:
			- Deleting a node with no children: simply remove the node from the tree.
			- Deleting a nod with right child: this is a bit more complicated, we replace the node we want
              to delete with it's in-order successor, then we delete the successor (which is the node we
              want to delete) like we do for a leaf (1st case). let us remember that in-order successor is
              effectively, for a node with 2 children, the left-most child of its right subtree.
			- Deleting a node in any other case (only left child), only left child: this is also simple, just
			 reconnect the parent of the toDelete to its child instead of the toDelete.
		   After the Insertion, the methods reheight() and rebalance() are called.
		   
reheight() - The method moves recursively through the tree and sets every node's correct height and balance

rebalance() - If during a modifying operation a height difference of more than one arises between two child
              subtrees, the parent subtree has to be "rebalanced". the method calls leftROtation and
              rightRotation if needed, and goes up the tree recursively until it
			  reaches the root.
			  
leftRotation() , rightRotation() - The right rotation operation is performed with Q as the root and hence is a
        right rotation on, or rooted at, Q. This operation results in a rotation of the tree in the clockwise
        direction. The inverse operation is the left rotation, which results in a movement in a
        counter-clockwise direction. The key to understanding how a rotation functions is to understand its
        constraints. In particular the order of the leaves of the tree (when read left to right for example)
        cannot change (another way to think of it is that the order that the leaves would be visited in an
        in-order traversal must be the same after the operation as before). Another constraint is the main
        property of a binary search tree, namely that the right child is greater than the parent and the left
        child is less than the parent. Notice that the right child of a left child of the root of a sub-tree
        can become the left child of the root, that itself becomes the right child of the "new" root in the
        rotated sub-tree, without violating either of those constraints.




====================================
= Answers to theoretical questions =
====================================


1. the sequence is 5, 3, 8, 2, 4, 7, 10, 1, 6, 9, 11, 12. 
   we figured out the sequence by first figuring out the order in which the nodes were inserted into the
   example tree. then we could generalize the procedure and find the right sequence for a tree with height 4.


2. for given input array of size n, the asymptotic running time complexity of the algorithm is O(n*logn).
   this is because the height of the tree is approximately log(n), and from the analyzing we did in DaSt for
   avl trees, we learned that when inserting n elements (which is the same as inserting an array, the we
   handled it), the total runtime complexity is O(n*logn).
   
   the best possible running time complexity is also theta(n*logn), and that is because even if we insert and
   array that is sorted in a way that won't need us rotate at all - it won't affect the runtime because for
   each insertion the rotations will require theta(1) operations so it will only add O(n) operations in total,
   not affecting the theta(n*logn) runtime complexity (n is negligible in comparison to n*logn).

   
3. the asymptotic running time complexity of the algorithm is O(n*logn). note that traveling through a
   balanced tree takes an asymptotic time complexity of O(n) which is done by the iterator. the way we
   implemented the copy constructor is that is iterating over the tree and inserting one by one. that causes
   every node we insert to be in the right-most place in the tree. that causes a-lot of rotations making the
   constant of the asymptotic runtime is rather high.

   with out implementation, in any case this will take O(n*logn). with other implementation the runtime
   complexity may be better by not using the iterator rather finding a way to insert directly and use less
   rotations or even.???????????????explain better


   
   
4. the asymptotic running time complexity of our implemantation of findMinNodes is O(h).

   O(h) isn't the best possible asymptotic running time complexity. the best achievable time complexity is
   O(logh). it can be achieved by using matrix multiplication to calculate the result - you can represent the
   desired result in height h as an element in a specific matrix, when raised to the power of h. It is
   possible to calculate a matrix in the power of h in O(logh) time complexity, using a recursive algorithm.
   